{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b34782b-ce3e-4be9-b11b-de1cc0c11c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Profiling fresh index build‚Ä¶\n",
      "‚úÖ Transcript fetched: 4076 segments in 2.76s\n",
      "üß© Windows created: 108 (‚âà75s each)\n",
      "\n",
      "‚è±Ô∏è  Timings (s):\n",
      "  transcript fetch     : 2.77\n",
      "  window grouping      : 0.00\n",
      "  docs creation        : 0.00\n",
      "  EMBEDDING (only)     : 48.09\n",
      "  FAISS add (flat)     : 0.034\n",
      "  Save native faiss    : 0.01\n",
      "  Save LangChain local : 0.07\n",
      "  ----------------------------\n",
      "  TOTAL (this profiler): 50.97\n",
      "\n",
      "üì¶ Sizes:\n",
      "  native index.faiss   : 0.17 MB\n",
      "  LC save_local folder : 0.32 MB\n",
      "\n",
      "Returned stats: {'t_fetch': 2.7651867866516113, 't_group': 0.0010025501251220703, 't_docs': 0.0010073184967041016, 't_embed_only': 48.09089159965515, 't_faiss_add': 0.03351998329162598, 't_save_native': 0.01379251480102539, 't_save_lc': 0.06844496726989746, 'total': 50.97384572029114, 'size_native': 165933, 'size_lc': 324431}\n"
     ]
    }
   ],
   "source": [
    "# --- Self-contained profiler for YT-RAG indexing ---\n",
    "import os, time, json, shutil, hashlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "# Embeddings / Vector store (LangChain community)\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS as LCFAISS\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Transcript API\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
    "\n",
    "# Try to import faiss (CPU or GPU)\n",
    "try:\n",
    "    import faiss  # faiss-cpu or faiss-gpu\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"FAISS is required. Install with `pip install faiss-cpu` (or faiss-gpu on Linux).\"\n",
    "    ) from e\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "CACHE_DIR = Path(\".yt_rag_cache\")\n",
    "TARGET_WINDOW_SECONDS = 75  # ~60‚Äì90s is good for long videos\n",
    "\n",
    "# -------------------- Helpers -------------------\n",
    "def _vid_dir(video_id: str) -> Path:\n",
    "    return CACHE_DIR / video_id\n",
    "\n",
    "def _ts(sec: float) -> str:\n",
    "    sec = int(sec)\n",
    "    h = sec // 3600\n",
    "    m = (sec % 3600) // 60\n",
    "    s = sec % 60\n",
    "    return f\"{h:d}:{m:02d}:{s:02d}\" if h else f\"{m:d}:{s:02d}\"\n",
    "\n",
    "def _dir_size_bytes(p: Path) -> int:\n",
    "    if not p.exists():\n",
    "        return 0\n",
    "    total = 0\n",
    "    for child in p.rglob(\"*\"):\n",
    "        if child.is_file():\n",
    "            try:\n",
    "                total += child.stat().st_size\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "    return total\n",
    "\n",
    "def fetch_transcript(video_id: str, languages: List[str] = [\"en\"]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fetch raw transcript segments (start, duration, text).\"\"\"\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        api = YouTubeTranscriptApi()\n",
    "        fetched = api.fetch(video_id, languages=languages)\n",
    "        data = fetched.to_raw_data()\n",
    "        print(f\"‚úÖ Transcript fetched: {len(data)} segments in {time.time()-t0:.2f}s\")\n",
    "        return data\n",
    "    except TranscriptsDisabled:\n",
    "        raise RuntimeError(\"No captions available for this video.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to fetch transcript: {e}\")\n",
    "\n",
    "def group_segments(segments: List[Dict[str, Any]], target_window_s: int = TARGET_WINDOW_SECONDS) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Greedy time-based grouping to ~target_window_s windows.\"\"\"\n",
    "    out = []\n",
    "    cur, start, end = [], None, None\n",
    "    for row in segments:\n",
    "        t0 = row[\"start\"]\n",
    "        t1 = row[\"start\"] + row.get(\"duration\", 0)\n",
    "        if start is None:\n",
    "            start = t0\n",
    "        end = t1\n",
    "        cur.append(row[\"text\"].strip())\n",
    "        if (end - start) >= target_window_s:\n",
    "            out.append({\"start\": start, \"end\": end, \"text\": \" \".join(cur).strip()})\n",
    "            cur, start, end = [], None, None\n",
    "    if cur:\n",
    "        out.append({\"start\": start, \"end\": end, \"text\": \" \".join(cur).strip()})\n",
    "    print(f\"üß© Windows created: {len(out)} (‚âà{target_window_s}s each)\")\n",
    "    return out\n",
    "\n",
    "def make_docs(video_id: str, windows: List[Dict[str, Any]]) -> List[Document]:\n",
    "    docs = []\n",
    "    for i, w in enumerate(windows):\n",
    "        meta = {\"video_id\": video_id, \"start\": w[\"start\"], \"end\": w[\"end\"], \"window_id\": i}\n",
    "        docs.append(Document(page_content=w[\"text\"], metadata=meta))\n",
    "    return docs\n",
    "\n",
    "# -------------------- Profiler -------------------\n",
    "def profile_build_index(video_id: str, time_langchain_save: bool = True):\n",
    "    vdir = _vid_dir(video_id)\n",
    "    idx_dir = vdir / \"faiss_prof\"\n",
    "    lc_dir  = idx_dir / \"lc\"              # LangChain save_local target\n",
    "    meta_fp = vdir / \"meta_prof.json\"\n",
    "\n",
    "    # Clean any old run safely\n",
    "    if idx_dir.exists():\n",
    "        shutil.rmtree(idx_dir, ignore_errors=True)\n",
    "\n",
    "    # Ensure parent dirs exist (creates .yt_rag_cache and video subfolder)\n",
    "    idx_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"üîé Profiling fresh index build‚Ä¶\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ---- Phase: transcript ‚Üí windows ‚Üí docs\n",
    "    t = time.time(); segs = fetch_transcript(video_id); t_fetch = time.time() - t\n",
    "    t = time.time(); wins = group_segments(segs);        t_group = time.time() - t\n",
    "    t = time.time(); docs = make_docs(video_id, wins);   t_docs  = time.time() - t\n",
    "\n",
    "    # ---- Phase: embeddings (timed explicitly)\n",
    "    t = time.time()\n",
    "    embeddings = FastEmbedEmbeddings()\n",
    "    texts = [d.page_content for d in docs]\n",
    "    vecs = embeddings.embed_documents(texts) if texts else []\n",
    "    t_embed_only = time.time() - t\n",
    "\n",
    "    # ---- Phase: FAISS add (native, timed)\n",
    "    t = time.time()\n",
    "    arr = np.array(vecs, dtype=\"float32\") if len(vecs) else np.zeros((0, 384), dtype=\"float32\")\n",
    "    if arr.ndim != 2:\n",
    "        # Ensure correct shape even for a single vector\n",
    "        arr = np.atleast_2d(arr).astype(\"float32\")\n",
    "    d = arr.shape[1] if arr.size else 384  # default dim guess if empty\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    if arr.size:\n",
    "        index.add(arr)\n",
    "    t_faiss_add = time.time() - t\n",
    "\n",
    "    # ---- Phase: Save (native faiss + simple metadata)\n",
    "    t = time.time()\n",
    "    idx_bin = idx_dir / \"index.faiss\"\n",
    "    faiss.write_index(index, str(idx_bin))\n",
    "    meta_out = idx_dir / \"metas.jsonl\"\n",
    "    with meta_out.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for ddoc in docs:\n",
    "            f.write(json.dumps(ddoc.metadata, ensure_ascii=False) + \"\\n\")\n",
    "    t_save_native = time.time() - t\n",
    "\n",
    "    # ---- Phase: Also time LangChain's save_local (optional; won't crash if unsupported)\n",
    "    t_save_lc = 0.0\n",
    "    if time_langchain_save:\n",
    "        t = time.time()\n",
    "        try:\n",
    "            lc_dir.mkdir(parents=True, exist_ok=True)\n",
    "            # Not all LC versions have FAISS.from_embeddings. We'll try two paths:\n",
    "            try:\n",
    "                # Newer LC path: create LC FAISS from precomputed embeddings\n",
    "                vs = LCFAISS.from_embeddings(\n",
    "                    embeddings=arr,\n",
    "                    embedding=embeddings,\n",
    "                    metadatas=[d.metadata for d in docs],\n",
    "                    texts=[d.page_content for d in docs],\n",
    "                )\n",
    "            except Exception:\n",
    "                # Fallback: manual constructor without re-embedding\n",
    "                from langchain.docstore.in_memory import InMemoryDocstore\n",
    "                from langchain.docstore.document import Document as LCDocument\n",
    "                import uuid\n",
    "                ids = [str(uuid.uuid4()) for _ in docs]\n",
    "                lc_docs = {i: LCDocument(page_content=d.page_content, metadata=d.metadata) for i, d in zip(ids, docs)}\n",
    "                docstore = InMemoryDocstore(lc_docs)\n",
    "                index2 = faiss.IndexFlatIP(d)\n",
    "                if arr.size:\n",
    "                    index2.add(arr)\n",
    "                vs = LCFAISS(\n",
    "                    embedding_function=embeddings,\n",
    "                    index=index2,\n",
    "                    docstore=docstore,\n",
    "                    index_to_docstore_id={i: id_ for i, id_ in enumerate(ids)},\n",
    "                )\n",
    "\n",
    "            LCFAISS.save_local(vs, str(lc_dir))\n",
    "            t_save_lc = time.time() - t\n",
    "        except Exception as e:\n",
    "            print(f\"‚ÑπÔ∏è Skipping LangChain save_local timing (not supported in this LC version): {e}\")\n",
    "            t_save_lc = 0.0\n",
    "\n",
    "    # ---- Sizes\n",
    "    size_native = idx_bin.stat().st_size if idx_bin.exists() else 0\n",
    "    size_lc = _dir_size_bytes(lc_dir)\n",
    "\n",
    "    total = time.time() - t0\n",
    "\n",
    "    print(f\"\\n‚è±Ô∏è  Timings (s):\"\n",
    "          f\"\\n  transcript fetch     : {t_fetch:.2f}\"\n",
    "          f\"\\n  window grouping      : {t_group:.2f}\"\n",
    "          f\"\\n  docs creation        : {t_docs:.2f}\"\n",
    "          f\"\\n  EMBEDDING (only)     : {t_embed_only:.2f}\"\n",
    "          f\"\\n  FAISS add (flat)     : {t_faiss_add:.3f}\"\n",
    "          f\"\\n  Save native faiss    : {t_save_native:.2f}\"\n",
    "          f\"\\n  Save LangChain local : {t_save_lc:.2f}\"\n",
    "          f\"\\n  ----------------------------\"\n",
    "          f\"\\n  TOTAL (this profiler): {total:.2f}\")\n",
    "\n",
    "    print(f\"\\nüì¶ Sizes:\"\n",
    "          f\"\\n  native index.faiss   : {size_native/1e6:.2f} MB\"\n",
    "          f\"\\n  LC save_local folder : {size_lc/1e6:.2f} MB\")\n",
    "\n",
    "    # Persist a tiny meta summary (optional)\n",
    "    try:\n",
    "        vdir.mkdir(parents=True, exist_ok=True)\n",
    "        with (meta_fp).open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"video_id\": video_id,\n",
    "                \"num_windows\": len(wins),\n",
    "                \"dim\": int(arr.shape[1]) if arr.size else 0,\n",
    "                \"native_index_path\": str(idx_dir / \"index.faiss\"),\n",
    "                \"lc_dir\": str(lc_dir),\n",
    "                \"timings\": {\n",
    "                    \"t_fetch\": t_fetch,\n",
    "                    \"t_group\": t_group,\n",
    "                    \"t_docs\": t_docs,\n",
    "                    \"t_embed_only\": t_embed_only,\n",
    "                    \"t_faiss_add\": t_faiss_add,\n",
    "                    \"t_save_native\": t_save_native,\n",
    "                    \"t_save_lc\": t_save_lc,\n",
    "                    \"total\": total\n",
    "                }\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return {\n",
    "        \"t_fetch\": t_fetch,\n",
    "        \"t_group\": t_group,\n",
    "        \"t_docs\": t_docs,\n",
    "        \"t_embed_only\": t_embed_only,\n",
    "        \"t_faiss_add\": t_faiss_add,\n",
    "        \"t_save_native\": t_save_native,\n",
    "        \"t_save_lc\": t_save_lc,\n",
    "        \"total\": total,\n",
    "        \"size_native\": size_native,\n",
    "        \"size_lc\": size_lc\n",
    "    }\n",
    "\n",
    "# -------- Example run --------\n",
    "# You can change the video id if you like.\n",
    "stats = profile_build_index(\"3qHkcs3kG44\", time_langchain_save=True)\n",
    "print(\"\\nReturned stats:\", stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeb1ba4-f47a-4a71-9e49-c2212a3f4439",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22703467-a82b-4ced-b10c-ee8aad56baa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QNA YT (rag)",
   "language": "python",
   "name": "rag-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
