{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61dc9b31-28e0-4d3f-a7c9-7446c31a9e83",
   "metadata": {},
   "source": [
    "# First Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5811e664-4754-495f-aafb-d35436c122f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Answer ===\n",
      "\n",
      "Yes, the topic of nuclear fusion is discussed in this video. \n",
      "\n",
      "The discussion about nuclear fusion starts with the speaker suggesting that we should be building nuclear fusion test plants on the moon. They mention that the problem with fission nuclear fission is that it was built with a bomb, and there are issues with dirty nukes, Fukushima, Three Mile Island, and Chernobyl. They also mention that we need a way to iterate on nuclear fission and eventually fusion to get them working safely, cleanly, and passively. \n",
      "\n",
      "The speaker also mentions that there are Gen 4 nuclear reactors that are passive fail-safe, meaning that when they fail, they fail into a safe state.\n"
     ]
    }
   ],
   "source": [
    "# yt_rag_gpu_cached.py\n",
    "# Drop-in for your original script: same QUESTION/LLM chain, faster indexing.\n",
    "\n",
    "# --- Imports ---\n",
    "import os, time, json, hashlib, shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "VIDEO_ID = \"3qHkcs3kG44\"\n",
    "QUESTION = (\n",
    "    \"Is the topic of nuclear fusion discussed in this video? \"\n",
    "    \"If yes, what exactly was discussed?\"\n",
    ")\n",
    "\n",
    "CACHE_DIR = Path(\".yt_rag_cache_0\")         # A folder path for all cache files.\n",
    "CACHE_DIR.mkdir(exist_ok=True)              # create folder if it doesn‚Äôt exist.\n",
    "EMB_CACHE_ROOT = CACHE_DIR / \"emb_cache_st\" # subfolder under CACHE_DIR, i.e .yt_rag_cache_0\n",
    "EMB_CACHE_ROOT.mkdir(exist_ok=True)         # make sure this exists, too.\n",
    "TARGET_WINDOW_SECONDS = 110                 # ~90‚Äì120s is a sweet spot for long YT videos\n",
    "\n",
    "# Retrieve N best-matching chunks for a query\n",
    "TOP_K = 4                                   \n",
    "\n",
    "# Sentence transformer model\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384-d, fast+solid\n",
    "BATCH_SIZE = 1024 # how many texts to embed at once. Large batch uses more GPU RAM.\n",
    "\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def _vid_dir(video_id: str) -> Path:\n",
    "    '''\n",
    "    A function that takes a video_id string and returns a Path\n",
    "    '''\n",
    "    v = CACHE_DIR / video_id                 # create a subfolder inside .yt_rag_cache named after the video ID.\n",
    "    v.mkdir(parents=True, exist_ok=True)     # ensure that folder exists.\n",
    "    return v                                 # give back the folder path.\n",
    "\n",
    "def _ts(sec: float) -> str:\n",
    "    '''\n",
    "    To convert seconds to H:MM:SS or M:SS string.\n",
    "    '''\n",
    "    sec = int(sec)                          # make sure its an integer\n",
    "    h = sec // 3600                         # to convert to hours, divide it by 3600\n",
    "    m = (sec % 3600) // 60                  # to convery to minutes, divide further by 60\n",
    "    s =sec % 60                             # modulo of seconds\n",
    "    return f\"{h:d}:{m:02d}:{s:02d}\" if h else f\"{m:d}:{s:02d}\"\n",
    "\n",
    "def fetch_transcript(video_id: str, languages: List[str] = [\"en\"]) -> List[Dict[str, Any]]:\n",
    "    '''\n",
    "    Function to download the transcript for a given video.\n",
    "    '''\n",
    "    try:\n",
    "        t0 = time.time()                                                                 # record the start time\n",
    "        fetched = YouTubeTranscriptApi().fetch(video_id, languages=languages)            # get transcript object\n",
    "        data = fetched.to_raw_data()                                                     # convert the transcript to a list of dicts\n",
    "        print(f\"‚úÖ Transcript fetched: {len(data)} segments in {time.time()-t0:.2f}s\")   # print time taken \n",
    "        return data                                                                      # Return the list of dicts\n",
    "    except TranscriptsDisabled:\n",
    "        raise SystemExit(\"No captions available for this video.\")                        # if no captions, exit with a message. \n",
    "    except Exception as e:\n",
    "        raise SystemExit(f\"Failed to fetch transcript: {e}\")                             # any other error, then exit with explanation\n",
    "\n",
    "def group_segments(segments: List[Dict[str, Any]], target_window_s: int = TARGET_WINDOW_SECONDS) -> List[Dict[str, Any]]:\n",
    "    '''\n",
    "    To combine many tiny caption segments into larger ‚Äúwindows‚Äù of ~target_window_s seconds. Please Google segment and window.\n",
    "    '''\n",
    "    out = []              # list of windows\n",
    "    cur = []              # current window‚Äôs list of text lines.\n",
    "    start = None          # start time of current window; start as None.\n",
    "    end = None            # End time of current window; start as None.\n",
    "    for row in segments:                                             # Loop over each row in the segments\n",
    "        t0 = row[\"start\"]                                            # Start time of the row\n",
    "        t1 = row[\"start\"] + row.get(\"duration\", 0)                   # End time of the row\n",
    "        if start is None:                                                      # if start is None\n",
    "            start = t0                                                                # Set it to start time of the row (first segment in window).\n",
    "        end = t1                                                     # Update End time of the row\n",
    "        cur.append(row[\"text\"].strip())                              # Add this segment‚Äôs text (row[\"text\"].strip()) to cur\n",
    "        if (end - start) >= target_window_s:                         # If the window duration (end - start) is >= target window seconds:\n",
    "            out.append({\"start\": start, \"end\": end, \"text\": \" \".join(cur).strip()}) # Add a new window dict to out with \"start\", \"end\", and concatenated \"text\"\n",
    "            cur = []                                                                          # Reset cur\n",
    "            start = None                                                                      # Reset start \n",
    "            end = None                                                                        # Reset end\n",
    "    if cur:                         # After loop, if cur still has leftover text, append final window.\n",
    "        out.append({\"start\": start, \n",
    "                    \"end\": end, \n",
    "                    \"text\": \" \".join(cur).strip()})\n",
    "        \n",
    "    print(f\"üß© Windows created: {len(out)} (‚âà{target_window_s}s each)\") # Print out\n",
    "    return out                                                           # Return out\n",
    "\n",
    "def make_docs(video_id: str, windows: List[Dict[str, Any]]) -> List[Document]:\n",
    "    '''\n",
    "    Converts each window into a LangChain Document.\n",
    "    '''\n",
    "    docs = []                        # An empty list\n",
    "    for i, w in enumerate(windows):  # Enumerate in windows list\n",
    "        meta = {                     # metadata about each window\n",
    "            \"video_id\": video_id,\n",
    "            \"start\": w[\"start\"],\n",
    "            \"end\": w[\"end\"],\n",
    "            \"window_id\": i,\n",
    "            \"time_range\": f\"{_ts(w['start'])}‚Äì{_ts(w['end'])}\",\n",
    "        }\n",
    "        docs.append(Document(page_content=w[\"text\"], metadata=meta)) # create Document with page_content as the text, and metadata\n",
    "    print(f\"üìÑ Document objects: {len(docs)}\") # print number of docs \n",
    "    return docs # return the list of docs.\n",
    "\n",
    "# -------------------- GPU embedding + cache --------------------\n",
    "def make_st_model(model_name: str = MODEL_NAME) -> SentenceTransformer:\n",
    "    '''\n",
    "    Decide whether to use GPU (cuda) or CPU with the model.\n",
    "    '''\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"          # If GPU is available, use it.\n",
    "    model = SentenceTransformer(model_name, device=device)           # Create the model (SentenceTransformer) with GPU/CPU\n",
    "    return model                                                     # Return the model.\n",
    "\n",
    "def _hash_text(t: str) -> str:\n",
    "    '''\n",
    "    Used to generate unique cache filenames for each text chunk.\n",
    "    '''\n",
    "    return hashlib.md5(t.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def embed_with_cache(model: SentenceTransformer, texts: List[str], cache_dir: Path, batch_size: int = BATCH_SIZE) -> np.ndarray:\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)              # ensure cache directory exists.  \n",
    "    vecs = []                                                 # will hold embeddings (or None for misses)\n",
    "    misses = []                                               # texts we don‚Äôt have cached embeddings for yet.\n",
    "    miss_idx = []                                             # positions of those texts.\n",
    "    for i, t in enumerate(texts):                             # Loop over all texts\n",
    "        fp = cache_dir / f\"{_hash_text(t)}.npy\"                     # fp is the file path for this text‚Äôs embedding, e.g., abcd1234.npy\n",
    "        if fp.exists():                                             # If file exists \n",
    "            vecs.append(np.load(fp))                                     # then, ‚Üí np.load(fp) and append to vecs.\n",
    "        else:                                                       # Else\n",
    "            vecs.append(None)                                            # Put None in vecs (placeholder)\n",
    "            misses.append(t)                                             # Append the text to misses.\n",
    "            miss_idx.append(i)                                           # Append index i to miss_idx.\n",
    "    if misses:                                                # only embed if there are texts not cached.\n",
    "        with torch.inference_mode():                          # disable gradient calculations (faster, less memory)\n",
    "            arr_new = model.encode(                                  # compute embeddings for:\n",
    "                misses,                                              # the misses list\n",
    "                batch_size=batch_size,                               # process in batches.\n",
    "                convert_to_numpy=True,                               # get NumPy array.\n",
    "                normalize_embeddings=True                            # L2-normalize, so cosine similarity becomes inner product\n",
    "            ).astype(\"float32\")                                      # Convert to float32, as FAISS expects float32 \n",
    "        for j, row in enumerate(arr_new):                     # Loop over each new embedding row\n",
    "            i = miss_idx[j]                                          # Get original index i from miss_idx\n",
    "            vecs[i] = row                                            # Put row into vecs[i].\n",
    "            np.save(cache_dir / f\"{_hash_text(texts[i])}.npy\", row)  # Save row to .npy file for future reuse.\n",
    "    return np.vstack(vecs).astype(\"float32\")                  # Stack vectors vertically into a single 2D array of shape (n_texts, embedding_dim).\n",
    "\n",
    "# -------------------- Index build/load --------------------\n",
    "def build_or_load_index(video_id: str, force_rebuild: bool = False):\n",
    "    vdir = _vid_dir(video_id)                                                         # Define directories and file paths for video cache folder.\n",
    "    idx_dir = vdir / \"faiss_st\"                                                       # ... inside that, FAISS-specific folder \n",
    "    meta_fp = vdir / \"meta_st.json\"                                                   # ... general metadata JSON file.\n",
    "    metas_path = idx_dir / \"metas.jsonl\"                                              # ... each document‚Äôs metadata in JSON Lines (.jsonl) format\n",
    "    idx_path = idx_dir / \"index.faiss\"                                                # ... FAISS index file itself.\n",
    "    cache_dir = EMB_CACHE_ROOT / video_id                                             # ... embeddings cache for this video\n",
    "\n",
    "    if force_rebuild and vdir.exists():                                               # If force_rebuild is True \n",
    "        shutil.rmtree(vdir, ignore_errors=True)                                               # delete the whole video directory\n",
    "    vdir.mkdir(parents=True, exist_ok=True)                                           # Then re-create it.\n",
    "\n",
    "    # Fast path\n",
    "    if idx_path.exists() and meta_fp.exists() and metas_path.exists():                # If the FAISS index and metadata files already exist\n",
    "        index = faiss.read_index(str(idx_path))                                               # Load FAISS index from disk.\n",
    "        metas = [json.loads(l) for l in metas_path.read_text(encoding=\"utf-8\").splitlines()]  # Read metadata lines and json.loads each line.\n",
    "        print(f\"‚ö° Loaded cached index: {idx_dir}\")                                            # Print message and return index, metas.\n",
    "        return index, metas\n",
    "\n",
    "    # Build fresh\n",
    "    t0 = time.time()                                                                  # If fast-path doesn‚Äôt happen, build from scratch.  \n",
    "    segs = fetch_transcript(video_id)                                                 # Record start time.\n",
    "    wins = group_segments(segs, target_window_s=TARGET_WINDOW_SECONDS)                # Get transcript segments.\n",
    "    docs = make_docs(video_id, wins)                                                  # Group into windows.\n",
    "\n",
    "    texts = [d.page_content for d in docs]                                            # Extract just the text from each document.\n",
    "    model = make_st_model(MODEL_NAME)                                                 # Create embedding model.\n",
    "\n",
    "    t = time.time()                                                                   # Time embedding process:\n",
    "    arr = embed_with_cache(model, texts, cache_dir=cache_dir, batch_size=BATCH_SIZE)  # NumPy array of embeddings (n, 384)\n",
    "    t_embed = time.time() - t                                                         # how long embedding took.\n",
    "\n",
    "    d = arr.shape[1]                                                                  # dimension of each vector.\n",
    "    index = faiss.IndexFlatIP(d)   # an index that uses inner product (IP) for similarity embeddings are L2-normalized => IP == cosine\n",
    "    index.add(arr)                                                                    # add all vectors to the index\n",
    "\n",
    "    idx_dir.mkdir(parents=True, exist_ok=True)                                        # Ensure index directory exists.\n",
    "    faiss.write_index(index, str(idx_path))                                           # Save FAISS index to disk.\n",
    "    with metas_path.open(\"w\", encoding=\"utf-8\") as f:                                 # Open metas.jsonl for writing:\n",
    "        for doc in docs:\n",
    "            f.write(json.dumps(doc.metadata, ensure_ascii=False) + \"\\n\")                   # For each doc, write its .metadata dict as a JSON line.\n",
    "    with meta_fp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"video_id\": video_id,\n",
    "            \"num_windows\": len(wins),\n",
    "            \"chunk_sec\": TARGET_WINDOW_SECONDS,\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"dim\": int(d),\n",
    "            \"embed_time_s\": round(t_embed, 3),\n",
    "        }, f, ensure_ascii=False, indent=2)                                          # Write a \"summary\" JSON file: video id, number of windows, etc.\n",
    "\n",
    "    print(f\"‚úÖ Index built in {time.time()-t0:.2f}s (embed {t_embed:.2f}s)‚Üí {idx_dir}\")\n",
    "    metas = [doc.metadata for doc in docs]                                           # Create metas list from documents.\n",
    "    return index, metas, model                                                       # Return index, metas, and model.\n",
    "\n",
    "# -------------------- Glue: build retriever & run chain --------------------\n",
    "# We‚Äôll store page_content alongside metadata so retrieval returns real docs.\n",
    "def build_index_and_retriever(video_id: str, force_rebuild: bool = False, k: int = TOP_K):\n",
    "    '''\n",
    "    Another function that builds or loads an index, but now stores full info (content + metadata) in metas_full.jsonl\n",
    "    '''\n",
    "    vdir = _vid_dir(video_id)\n",
    "    idx_dir = vdir / \"faiss_st\"\n",
    "    metas_path = idx_dir / \"metas_full.jsonl\"                      # store content + meta here\n",
    "    idx_path = idx_dir / \"index.faiss\"\n",
    "\n",
    "    if force_rebuild and vdir.exists():                            # Same logic: if force_rebuild, delete everything for this video; then recreate.\n",
    "        shutil.rmtree(vdir, ignore_errors=True)\n",
    "    vdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # If not present, build fresh with full metas\n",
    "    if not (idx_path.exists() and metas_path.exists()):            # If either index or metas_full.jsonl is missing, build from scratch.\n",
    "        # Fresh build (repeats a bit of code for clarity)\n",
    "        segs = fetch_transcript(video_id)                                   \n",
    "        wins = group_segments(segs, \n",
    "                              target_window_s=TARGET_WINDOW_SECONDS)\n",
    "        docs = make_docs(video_id, wins)                                    \n",
    "\n",
    "        texts = [d.page_content for d in docs]\n",
    "        model = make_st_model(MODEL_NAME)\n",
    "        arr = embed_with_cache(model,                              # Same embedding logic as above.\n",
    "                               texts, \n",
    "                               cache_dir=EMB_CACHE_ROOT / video_id, \n",
    "                               batch_size=BATCH_SIZE) \n",
    "\n",
    "        d = arr.shape[1]\n",
    "        index = faiss.IndexFlatIP(d)             \n",
    "        index.add(arr)                                             # Create FAISS index and add vectors.\n",
    "\n",
    "        idx_dir.mkdir(parents=True, exist_ok=True)\n",
    "        faiss.write_index(index, str(idx_path))\n",
    "        with metas_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            for ddoc in docs:\n",
    "                # keep content for LC chain\n",
    "                payload = {\"page_content\": ddoc.page_content, \n",
    "                           \"metadata\": ddoc.metadata}\n",
    "                f.write(json.dumps(payload,                      # Write metas_full.jsonl where each line is: {\"page_content\": \"...\", \"metadata\": {...}}\n",
    "                                   ensure_ascii=False) + \"\\n\")    \n",
    "        # Return index, metas_full, and model.\n",
    "        return index, [json.loads(l) for l in metas_path.read_text(encoding=\"utf-8\").splitlines()], model\n",
    "\n",
    "    # If files already exist, Load path\n",
    "    index = faiss.read_index(str(idx_path))                                                    # Load FAISS index\n",
    "    metas_full = [json.loads(l) for l in metas_path.read_text(encoding=\"utf-8\").splitlines()]  # Load metas_full.jsonl into a list of dicts.\n",
    "    model = make_st_model(MODEL_NAME)                                                          # Build model\n",
    "    return index, metas_full, model                                                            # Return them.\n",
    "\n",
    "class LCStyleRetriever:\n",
    "    \"\"\"A tiny retriever that returns langchain.Document objects.\"\"\"\n",
    "    def __init__(self, index: faiss.Index, metas_full: List[Dict[str, Any]], model: SentenceTransformer, k: int = TOP_K):\n",
    "        '''\n",
    "        __init__ stores the index, metadata, model, and k on self.\n",
    "        '''\n",
    "        self.index = index\n",
    "        self.metas_full = metas_full  # list of {\"page_content\":..., \"metadata\":...}\n",
    "        self.model = model\n",
    "        self.k = k\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        with torch.inference_mode():\n",
    "            q = self.model.encode([query],                               # Embeds the query into q.\n",
    "                                  batch_size=1, \n",
    "                                  convert_to_numpy=True, \n",
    "                                  normalize_embeddings=True).astype(\"float32\")\n",
    "        D, I = self.index.search(q, self.k)                              # Performs search in FAISS index: D are distances/similarities; I are indices.\n",
    "        docs = []                                                        # empty list.\n",
    "        for idx in I[0]:                                                 # Loop over indices in I[0]:\n",
    "            if idx == -1:                                                           # Skip invalid index -1.\n",
    "                continue\n",
    "            payload = self.metas_full[idx]                                          # get the content + metadata dict\n",
    "            docs.append(Document(page_content=payload[\"page_content\"], # Create a Document with page_content and metadata.\n",
    "                                 metadata=payload[\"metadata\"]))\n",
    "        return docs                # Return the list of documents.\n",
    "\n",
    "\n",
    "# -------------------- Build retriever + LLM chain (matches your original shape) --------------------\n",
    "\n",
    "# Call build_index_and_retriever for the chosen VIDEO_ID and Get back index, metas_full, and model.\n",
    "index, metas_full, model = build_index_and_retriever(VIDEO_ID, force_rebuild=False, k=TOP_K)\n",
    "\n",
    "# Create an instance of LCStyleRetriever using these.\n",
    "retriever = LCStyleRetriever(index, metas_full, model, k=TOP_K)\n",
    "\n",
    "# PromptTemplate defines how the final prompt to the LLM will look.\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], # input_variables ‚Äì the placeholders you‚Äôll fill in ({context} and {question}).\n",
    "    template=( # template ‚Äì formatted string with those placeholders.\n",
    "        \"You are a concise, helpful assistant. Use ONLY the context to answer.\\n\\n\"\n",
    "        \"Context:\\n{context}\\n\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"If the answer isn't in the context, say you can't find it.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.2) # Creates the ChatGroq LLM client, with model and temperature.\n",
    "\n",
    "def format_docs(retrieved_docs: List[Document]) -> str:\n",
    "    '''\n",
    "    Given a list of Documents, join their page_content into one big string, separated by two newlines.\n",
    "    '''\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "parallel = RunnableParallel({\n",
    "    \"context\": RunnableLambda(lambda q: retriever.get_relevant_documents(q)) | RunnableLambda(format_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "})\n",
    "parser = StrOutputParser()                               # ensures we end with a plain string.\n",
    "main_chain = parallel | prompt | llm | parser            # The pipeline\n",
    "\n",
    "# --- Run ---\n",
    "answer = main_chain.invoke(QUESTION)\n",
    "print(\"\\n=== Answer ===\\n\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24704c9c-15a8-4578-be75-2cefb13dfff0",
   "metadata": {},
   "source": [
    "# Other versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca19016e-14a6-4b15-866d-fe35bcea4f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ No existing index found. Building a new one...\n",
      "‚úÖ Transcript fetched: 4076 segments in 2.86s\n",
      "üß© Windows created: 73 (‚âà110s each)\n",
      "üìÑ Document objects: 73\n",
      "üß† Loading SentenceTransformer on: cuda\n",
      "üßÆ Embedding 73 new texts (not in cache yet)...\n",
      "‚úÖ New index built and saved.\n",
      "\n",
      "=== Answer ===\n",
      "\n",
      "Yes, the topic of nuclear fusion is discussed in this video. Specifically, the conversation mentions that nuclear fusion is a technology that is not far from working, and that building nuclear fusion test plants on the moon could be a possibility.\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Imports --------------------\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "# Load environment variables from .env (this should contain GROQ_API_KEY)\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# The YouTube video you want to query\n",
    "VIDEO_ID = \"3qHkcs3kG44\"\n",
    "\n",
    "# The question you want to ask about this video\n",
    "QUESTION = (\n",
    "    \"Is the topic of nuclear fusion discussed in this video? \"\n",
    "    \"If yes, what exactly was discussed?\"\n",
    ")\n",
    "\n",
    "# Root directory to store all cached data (transcript/embeddings/index)\n",
    "CACHE_DIR = Path(\".yt_rag_cache_1\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Root directory for embedding cache (per video)\n",
    "EMB_CACHE_ROOT = CACHE_DIR / \"emb_cache_st\"\n",
    "EMB_CACHE_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "# How long each transcript window (chunk) should be, in seconds\n",
    "TARGET_WINDOW_SECONDS = 110   # around 90‚Äì120 seconds is good for long videos\n",
    "\n",
    "# How many top matching chunks to retrieve for each query\n",
    "TOP_K = 4\n",
    "\n",
    "# SentenceTransformer model (text ‚Üí vector)\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384-dim, fast and solid\n",
    "\n",
    "# Batch size for embedding (reduce if GPU memory is small)\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "\n",
    "# -------------------- Helper functions --------------------\n",
    "def get_video_dir(video_id):\n",
    "    \"\"\"\n",
    "    Return the cache directory for a specific video.\n",
    "    Example: .yt_rag_cache/3qHkcs3kG44\n",
    "    \"\"\"\n",
    "    vdir = CACHE_DIR / video_id\n",
    "    vdir.mkdir(parents=True, exist_ok=True)\n",
    "    return vdir\n",
    "\n",
    "\n",
    "def seconds_to_timestamp(sec):\n",
    "    \"\"\"\n",
    "    Convert seconds (float) to a H:MM:SS or M:SS string.\n",
    "    Example: 75.3 -> \"1:15\"\n",
    "    \"\"\"\n",
    "    sec = int(sec)\n",
    "    h = sec // 3600\n",
    "    m = (sec % 3600) // 60\n",
    "    s = sec % 60\n",
    "    if h:\n",
    "        return f\"{h:d}:{m:02d}:{s:02d}\"\n",
    "    else:\n",
    "        return f\"{m:d}:{s:02d}\"\n",
    "\n",
    "\n",
    "def fetch_transcript(video_id, languages=[\"en\"]):\n",
    "    \"\"\"\n",
    "    Download the transcript (subtitles) for a given YouTube video.\n",
    "    Returns a list of segments, where each segment is a dict containing:\n",
    "    - 'start': when the segment starts (in seconds)\n",
    "    - 'duration': how long it lasts\n",
    "    - 'text': the subtitle text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        fetched = YouTubeTranscriptApi().fetch(video_id, languages=languages)\n",
    "        data = fetched.to_raw_data()\n",
    "        print(f\"‚úÖ Transcript fetched: {len(data)} segments in {time.time() - t0:.2f}s\")\n",
    "        return data\n",
    "    except TranscriptsDisabled:\n",
    "        raise SystemExit(\"No captions available for this video.\")\n",
    "    except Exception as e:\n",
    "        raise SystemExit(f\"Failed to fetch transcript: {e}\")\n",
    "\n",
    "\n",
    "def group_segments_into_windows(segments, target_window_s=TARGET_WINDOW_SECONDS):\n",
    "    \"\"\"\n",
    "    Combine many small transcript segments into larger \"windows\" of around\n",
    "    target_window_s seconds each. \n",
    "\n",
    "    Returns a list of dicts like:\n",
    "    {\n",
    "        \"start\": ...,\n",
    "        \"end\": ...,\n",
    "        \"text\": \"combined transcript text\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    current_texts = []\n",
    "    current_start = None\n",
    "    current_end = None\n",
    "\n",
    "    for row in segments:\n",
    "        start = row[\"start\"]\n",
    "        end = row[\"start\"] + row.get(\"duration\", 0)\n",
    "\n",
    "        # If this is the first segment in the current window\n",
    "        if current_start is None:\n",
    "            current_start = start\n",
    "\n",
    "        current_end = end\n",
    "        current_texts.append(row[\"text\"].strip())\n",
    "\n",
    "        # If we've reached or exceeded the target window length, close this window\n",
    "        if (current_end - current_start) >= target_window_s:\n",
    "            window_text = \" \".join(current_texts).strip()\n",
    "            windows.append({\"start\": current_start, \"end\": current_end, \"text\": window_text})\n",
    "            # Reset for the next window\n",
    "            current_texts = []\n",
    "            current_start = None\n",
    "            current_end = None\n",
    "\n",
    "    # If any leftover segments remain, make a final window\n",
    "    if current_texts:\n",
    "        window_text = \" \".join(current_texts).strip()\n",
    "        windows.append({\"start\": current_start, \"end\": current_end, \"text\": window_text})\n",
    "\n",
    "    print(f\"üß© Windows created: {len(windows)} (‚âà{target_window_s}s each)\")\n",
    "    return windows\n",
    "\n",
    "\n",
    "def make_documents_from_windows(video_id, windows):\n",
    "    \"\"\"\n",
    "    Convert each window into a LangChain Document, storing helpful metadata\n",
    "    (like start/end times and a human-readable time range).\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for i, w in enumerate(windows):\n",
    "        meta = {\n",
    "            \"video_id\": video_id,\n",
    "            \"start\": w[\"start\"],\n",
    "            \"end\": w[\"end\"],\n",
    "            \"window_id\": i,\n",
    "            \"time_range\": f\"{seconds_to_timestamp(w['start'])}‚Äì{seconds_to_timestamp(w['end'])}\",\n",
    "        }\n",
    "        docs.append(Document(page_content=w[\"text\"], metadata=meta))\n",
    "\n",
    "    print(f\"üìÑ Document objects: {len(docs)}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "# -------------------- Embedding model + caching --------------------\n",
    "def make_sentence_transformer_model(model_name=MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Create a SentenceTransformer model, placing it on GPU if available,\n",
    "    otherwise on CPU.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"üß† Loading SentenceTransformer on: {device}\")\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def hash_text_to_filename(text):\n",
    "    \"\"\"\n",
    "    Create a stable hash from text so we can use it as a cache filename.\n",
    "    This avoids illegal characters and super long filenames.\n",
    "    \"\"\"\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def embed_texts_with_cache(model, texts, cache_dir, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Embed a list of texts using SentenceTransformer, but use a local cache\n",
    "    directory so that repeated runs do NOT have to embed the same text again.\n",
    "\n",
    "    Steps:\n",
    "    - For each text, check if its embedding .npy file exists in the cache.\n",
    "    - If yes ‚Üí load it.\n",
    "    - If no ‚Üí remember it as a \"miss\" and embed later.\n",
    "    - After embedding the misses, save them to disk and place them in the\n",
    "      right positions in the output array.\n",
    "\n",
    "    Returns a NumPy array of shape (num_texts, embedding_dim).\n",
    "    \"\"\"\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    vectors = []      # will hold embeddings or None\n",
    "    misses = []       # texts that are not cached yet\n",
    "    miss_indices = [] # positions of those texts in the original list\n",
    "\n",
    "    # First pass: find which texts already have cached embeddings\n",
    "    for i, text in enumerate(texts):\n",
    "        filename = hash_text_to_filename(text) + \".npy\"\n",
    "        filepath = cache_dir / filename\n",
    "\n",
    "        if filepath.exists():\n",
    "            # Cache hit: load embedding from disk\n",
    "            vectors.append(np.load(filepath))\n",
    "        else:\n",
    "            # Cache miss: we will embed this later\n",
    "            vectors.append(None)\n",
    "            misses.append(text)\n",
    "            miss_indices.append(i)\n",
    "\n",
    "    # Second pass: embed all \"missing\" texts in one or more batches\n",
    "    if misses:\n",
    "        print(f\"üßÆ Embedding {len(misses)} new texts (not in cache yet)...\")\n",
    "        with torch.inference_mode():\n",
    "            arr_new = model.encode(\n",
    "                misses,\n",
    "                batch_size=batch_size,\n",
    "                convert_to_numpy=True,\n",
    "                normalize_embeddings=True,  # L2-normalize for cosine similarity\n",
    "            ).astype(\"float32\")\n",
    "\n",
    "        # Store each new embedding both in memory and on disk\n",
    "        for j, vec in enumerate(arr_new):\n",
    "            original_index = miss_indices[j]\n",
    "            vectors[original_index] = vec\n",
    "            filename = hash_text_to_filename(texts[original_index]) + \".npy\"\n",
    "            filepath = cache_dir / filename\n",
    "            np.save(filepath, vec)\n",
    "\n",
    "    # Stack all vectors into a single 2D array (num_texts, dim)\n",
    "    embeddings = np.vstack(vectors).astype(\"float32\")\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# -------------------- Build or load FAISS index + metadata --------------------\n",
    "def build_index_and_retriever(video_id, force_rebuild=False, k=TOP_K):\n",
    "    \"\"\"\n",
    "    Main function that:\n",
    "    - Downloads transcript (if needed)\n",
    "    - Groups into windows\n",
    "    - Builds or loads embeddings\n",
    "    - Builds or loads a FAISS index\n",
    "    - Returns:\n",
    "        - index: FAISS index\n",
    "        - metas_full: list of dictionaries with \"page_content\" and \"metadata\"\n",
    "        - model: SentenceTransformer model\n",
    "    \"\"\"\n",
    "    video_dir = get_video_dir(video_id)\n",
    "    index_dir = video_dir / \"faiss_st\"\n",
    "    metas_path = index_dir / \"metas_full.jsonl\"  # stores content + metadata\n",
    "    index_path = index_dir / \"index.faiss\"\n",
    "    cache_dir = EMB_CACHE_ROOT / video_id\n",
    "\n",
    "    if force_rebuild and video_dir.exists():\n",
    "        print(\"‚ôªÔ∏è force_rebuild=True ‚Üí deleting old cache for this video...\")\n",
    "        shutil.rmtree(video_dir, ignore_errors=True)\n",
    "        video_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    index_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # If we do NOT already have an index and metadata, build them from scratch\n",
    "    if not (index_path.exists() and metas_path.exists()):\n",
    "        print(\"üì¶ No existing index found. Building a new one...\")\n",
    "\n",
    "        # 1. Fetch transcript\n",
    "        segments = fetch_transcript(video_id)\n",
    "\n",
    "        # 2. Group into windows\n",
    "        windows = group_segments_into_windows(segments, target_window_s=TARGET_WINDOW_SECONDS)\n",
    "\n",
    "        # 3. Turn windows into Documents\n",
    "        docs = make_documents_from_windows(video_id, windows)\n",
    "\n",
    "        # 4. Extract raw texts and embed them (with caching)\n",
    "        texts = [doc.page_content for doc in docs]\n",
    "        model = make_sentence_transformer_model(MODEL_NAME)\n",
    "        embeddings = embed_texts_with_cache(model, texts, cache_dir=cache_dir, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # 5. Build FAISS index (inner product == cosine because we normalized embeddings)\n",
    "        dim = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        index.add(embeddings)\n",
    "\n",
    "        # 6. Save FAISS index to disk\n",
    "        faiss.write_index(index, str(index_path))\n",
    "\n",
    "        # 7. Save \"full\" metadata (page_content + metadata) to JSON Lines file\n",
    "        metas_full = []\n",
    "        with metas_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            for doc in docs:\n",
    "                payload = {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
    "                metas_full.append(payload)\n",
    "                f.write(json.dumps(payload, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(\"‚úÖ New index built and saved.\")\n",
    "        return index, metas_full, model\n",
    "\n",
    "    # If we DO have an index and metadata, just load them\n",
    "    print(\"‚ö° Found existing index. Loading from disk...\")\n",
    "    index = faiss.read_index(str(index_path))\n",
    "\n",
    "    metas_full = []\n",
    "    for line in metas_path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        metas_full.append(json.loads(line))\n",
    "\n",
    "    model = make_sentence_transformer_model(MODEL_NAME)\n",
    "\n",
    "    return index, metas_full, model\n",
    "\n",
    "\n",
    "# -------------------- Simple retriever class --------------------\n",
    "class LCStyleRetriever:\n",
    "    \"\"\"\n",
    "    A tiny retriever that:\n",
    "    - stores the FAISS index\n",
    "    - stores the SentenceTransformer model\n",
    "    - stores content + metadata\n",
    "    and returns LangChain Document objects for a given query.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, index, metas_full, model, k=TOP_K):\n",
    "        self.index = index\n",
    "        self.metas_full = metas_full  # list of {\"page_content\": ..., \"metadata\": ...}\n",
    "        self.model = model\n",
    "        self.k = k\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        \"\"\"\n",
    "        Embed the query, search the FAISS index for top-k nearest neighbors,\n",
    "        then return a list of LangChain Document objects.\n",
    "        \"\"\"\n",
    "        with torch.inference_mode():\n",
    "            query_vec = self.model.encode(\n",
    "                [query],\n",
    "                batch_size=1,\n",
    "                convert_to_numpy=True,\n",
    "                normalize_embeddings=True,\n",
    "            ).astype(\"float32\")\n",
    "\n",
    "        # FAISS search: returns (distances, indices)\n",
    "        distances, indices = self.index.search(query_vec, self.k)\n",
    "\n",
    "        docs = []\n",
    "        for idx in indices[0]:\n",
    "            if idx == -1:  # just a safety check; usually not needed\n",
    "                continue\n",
    "            payload = self.metas_full[idx]\n",
    "            doc = Document(page_content=payload[\"page_content\"], metadata=payload[\"metadata\"])\n",
    "            docs.append(doc)\n",
    "\n",
    "        return docs\n",
    "\n",
    "\n",
    "# -------------------- Build retriever + LLM chain --------------------\n",
    "# Build or load the FAISS index and create a retriever\n",
    "index, metas_full, st_model = build_index_and_retriever(\n",
    "    VIDEO_ID,\n",
    "    force_rebuild=False,  # set True if you change chunking/model and want a fresh index\n",
    "    k=TOP_K,\n",
    ")\n",
    "\n",
    "retriever = LCStyleRetriever(index, metas_full, st_model, k=TOP_K)\n",
    "\n",
    "# Define the prompt template that the LLM will see\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"You are a concise, helpful assistant. Use ONLY the context to answer.\\n\\n\"\n",
    "        \"Context:\\n{context}\\n\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"If the answer isn't in the context, say you can't find it.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create the Groq LLM client\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.2)\n",
    "\n",
    "\n",
    "def format_docs(retrieved_docs):\n",
    "    \"\"\"\n",
    "    Take a list of LangChain Document objects and turn them into a single\n",
    "    string that will be passed as 'context' to the LLM.\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "\n",
    "# Create a small pipeline that:\n",
    "# 1. Takes the question as input.\n",
    "# 2. In parallel:\n",
    "#    - For \"context\": runs the retriever, then formats the docs.\n",
    "#    - For \"question\": just passes the question through untouched.\n",
    "parallel = RunnableParallel(\n",
    "    {\n",
    "        \"context\": RunnableLambda(lambda q: retriever.get_relevant_documents(q))\n",
    "        | RunnableLambda(format_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Parse the LLM's output as a plain string\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Combine everything:\n",
    "# question ‚Üí parallel ‚Üí prompt ‚Üí llm ‚Üí parser ‚Üí final answer string\n",
    "main_chain = parallel | prompt | llm | parser\n",
    "\n",
    "\n",
    "# -------------------- Run the chain --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the full RAG pipeline on the QUESTION and print the answer\n",
    "    answer = main_chain.invoke(QUESTION)\n",
    "    print(\"\\n=== Answer ===\\n\")\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385c90d-6a2c-437c-94e4-b892366cef98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QNA YT (rag)",
   "language": "python",
   "name": "rag-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
